from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
import logging
import pandas as pd
import time
import re
from datetime import datetime
import json

class GazprombankScraper:
    def __init__(self, headless=True):
        self.driver = None
        self.setup_driver(headless)
        
    def setup_driver(self, headless):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –±—Ä–∞—É–∑–µ—Ä–∞"""
        chrome_options = Options()
        if headless:
            chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
        
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.wait = WebDriverWait(self.driver, 20)
    
    def clean_text(self, text):
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        if not text:
            return ""
        
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        text = re.sub(r'¬©.*?$', '', text)
        text = re.sub(r'–û—Ç–≤–µ—Ç –±–∞–Ω–∫–∞.*', '', text)
        text = re.sub(r'–ü–æ–∫–∞–∑–∞—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é', '', text)
        
        return text
    
    def parse_date(self, date_text):
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        try:
            months = {
                '—è–Ω–≤–∞—Ä—è': '01', '—Ñ–µ–≤—Ä–∞–ª—è': '02', '–º–∞—Ä—Ç–∞': '03',
                '–∞–ø—Ä–µ–ª—è': '04', '–º–∞—è': '05', '–∏—é–Ω—è': '06',
                '–∏—é–ª—è': '07', '–∞–≤–≥—É—Å—Ç–∞': '08', '—Å–µ–Ω—Ç—è–±—Ä—è': '09',
                '–æ–∫—Ç—è–±—Ä—è': '10', '–Ω–æ—è–±—Ä—è': '11', '–¥–µ–∫–∞–±—Ä—è': '12'
            }
            
            for ru_month, num_month in months.items():
                if ru_month in date_text:
                    date_text = date_text.replace(ru_month, num_month)
                    break
            
            date_obj = datetime.strptime(date_text, '%d %m %Y')
            return date_obj.strftime('%Y-%m-%d')
            
        except Exception as e:
            logging.exception(e)
            return date_text  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å
    
    def extract_review_data(self, review_element):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –æ—Ç–∑—ã–≤–∞"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º BeautifulSoup –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML
            html = review_element.get_attribute('outerHTML')
            soup = BeautifulSoup(html, 'lxml')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            author = soup.find('div', class_='response-page__header__author')
            author = author.text.strip() if author else '–ê–Ω–æ–Ω–∏–º'
            
            date = soup.find('div', class_='response-page__header__date')
            date = date.text.strip() if date else '–ù–µ —É–∫–∞–∑–∞–Ω–∞'
            date = self.parse_date(date)
            
            # –¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞
            text_div = soup.find('div', class_='response-page__text')
            text = text_div.text.strip() if text_div else ''
            text = self.clean_text(text)
            
            # –†–µ–π—Ç–∏–Ω–≥ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            rating = 0
            rating_div = soup.find('div', class_='response-page__header__rating')
            if rating_div:
                stars = rating_div.find_all('svg', class_='icon-star')
                rating = len([star for star in stars if 'icon-star_active' in str(star)])
            
            # –ü—Ä–æ–¥—É–∫—Ç (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
            product = soup.find('a', class_='response-page__header__product')
            product = product.text.strip() if product else '–ù–µ —É–∫–∞–∑–∞–Ω'
            
            return {
                'author': author,
                'date': date,
                'rating': rating,
                'product': product,
                'text': text,
                'source': 'banki.ru'
            }
            
        except Exception as e:
            logging.info(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}")
            return None
    
    def scrape_gazprombank_reviews(self, max_pages=10):
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫—Ä–µ–π–ø–∏–Ω–≥–∞"""
        url = "https://www.banki.ru/services/responses/bank/gazprombank/"
        all_reviews = []
        
        try:
            logging.info("–ó–∞–ø—É—Å–∫–∞–µ–º —Å–∫—Ä–µ–π–ø–∏–Ω–≥ –ì–∞–∑–ø—Ä–æ–º–±–∞–Ω–∫–∞...")
            self.driver.get(url)
            
            try:
                cookie_btn = self.wait.until(
                    EC.element_to_be_clickable((By.CLASS_NAME, "cookie-warning__button"))
                )
                cookie_btn.click()
                logging.info("Cookies –ø—Ä–∏–Ω—è—Ç—ã")
                time.sleep(1)
            except Exception as e:
                logging.exception(e)
                pass
            
            current_page = 1
            
            while current_page <= max_pages:
                logging.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {current_page}...")
                
                self.wait.until(
                    EC.presence_of_element_located((By.CLASS_NAME, "response-page"))
                )
                
                self.scroll_page()
                time.sleep(2)
                
                review_elements = self.driver.find_elements(By.CLASS_NAME, "response-page")
                logging.info(f"–ù–∞–π–¥–µ–Ω–æ –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(review_elements)}")
                
                for element in review_elements:
                    review_data = self.extract_review_data(element)
                    if review_data:
                        all_reviews.append(review_data)
                
                if not self.go_to_next_page() or current_page >= max_pages:
                    break
                
                current_page += 1
                time.sleep(3)
            
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–µ–π–ø–∏–Ω–≥–µ: {e}")

        finally:
            self.driver.quit()
        
        return all_reviews
    
    def scroll_page(self):
        """–ü—Ä–æ–∫—Ä—É—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)
        except Exception as e:
            logging.exception(e)
            pass
    
    def go_to_next_page(self):
        """–ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É"""
        try:
            next_buttons = self.driver.find_elements(By.CSS_SELECTOR, ".paginator__item-next:not(.paginator__item-disabled)")
            
            if next_buttons:
                next_buttons[0].click()
                time.sleep(2)
                return True
            return False
            
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–µ–π–ø–∏–Ω–≥–µ: {e}")
    
    def save_results(self, reviews, filename_prefix="gazprombank_reviews"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        df = pd.DataFrame(reviews)
        

        csv_file = f"{filename_prefix}.csv"
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        

        json_file = f"{filename_prefix}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(reviews, f, ensure_ascii=False, indent=2)
        

        excel_file = f"{filename_prefix}.xlsx"
        df.to_excel(excel_file, index=False)
        
        logging.info("\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        logging.info(f"CSV: {csv_file}")
        logging.info(f"JSON: {json_file}")
        logging.info(f"Excel: {excel_file}")
        
        return df


if __name__ == "__main__":
    logging.info("üöÄ –ó–∞–ø—É—Å–∫ —Å–∫—Ä–µ–π–ø–µ—Ä–∞ –ì–∞–∑–ø—Ä–æ–º–±–∞–Ω–∫–∞...")

    scraper = GazprombankScraper(headless=False)
    

    reviews = scraper.scrape_gazprombank_reviews(max_pages=10)
    

    if reviews:
        df = scraper.save_results(reviews)
        logging.info(f"\n‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–æ–±—Ä–∞–Ω–æ –æ—Ç–∑—ã–≤–æ–≤: {len(reviews)}")
        logging.info("\nüìä –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:")
        logging.info(df[['date', 'rating', 'product', 'text']].head(3).to_string())
    else:
        logging.info("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –æ—Ç–∑—ã–≤—ã")